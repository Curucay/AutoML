
from __future__ import annotations
from dataclasses import dataclass
from typing import Tuple, List
import io
import streamlit as st
import pandas as pd
import polars as pl
import numpy as np
from pandas.api.types import (
    is_numeric_dtype,
    is_datetime64_any_dtype,
    is_datetime64tz_dtype,
)

# IterativeImputer'Ä± "experimental" (deneysel) olarak etkinleÅŸtir:
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import KNNImputer, IterativeImputer
from sklearn.preprocessing import StandardScaler

@dataclass
class DataProfile:
    n_rows: int
    n_cols: int
    mem_usage_mb: float
    missing_total: int
    missing_ratio: float
    numeric_cols: List[str]
    categorical_cols: List[str]
    datetime_cols: List[str]
    sample: pd.DataFrame

class DataUtils:
    @staticmethod
    def read_any(file_name: str, file_bytes: bytes, **kwargs) -> pl.DataFrame:
        name = (file_name or "").lower()
        buffer = io.BytesIO(file_bytes)

        if name.endswith(".csv"):
            return pl.read_csv(buffer, **kwargs)
        if name.endswith(".parquet"):
            return pl.read_parquet(buffer, **kwargs)
        if name.endswith(".xlsx") or name.endswith(".xls"):
            import openpyxl
            import pandas as pd
            df = pd.read_excel(buffer, **kwargs)
            return pl.from_pandas(df)
        raise ValueError(f"Desteklenmeyen uzantÄ±: {name}")

    @staticmethod
    def sanitize_df(df: pl.DataFrame) -> pl.DataFrame:
        """
        "Unnamed" ile baÅŸlayan kolonlarÄ± temizler ve tÃ¼m kolon adlarÄ±ndaki
        gereksiz boÅŸluklarÄ± kaldÄ±rÄ±r.
        """
        # Kolon isimlerini dÃ¼zenle
        clean_cols = [str(c).strip() for c in df.columns]

        # Yeni kolon isimlerini uygula
        df = df.rename({old: new for old, new in zip(df.columns, clean_cols)})

        # "Unnamed" ile baÅŸlayanlarÄ± filtrele
        unnamed_cols = [c for c in df.columns if str(c).startswith("Unnamed")]
        if unnamed_cols:
            df = df.drop(unnamed_cols)

        return df

    @staticmethod
    def infer_dtypes(
            df: pl.DataFrame,
            datetime_guess: bool = True,
            coerce_threshold: float = 0.8,
            normalize_tz_to_naive_utc: bool = False,
            protected_cols: list[str] | None = None,
            protect_id_like_names: bool = True,
    ) -> pl.DataFrame:
        """
        Kolon veri tiplerini otomatik olarak tahmin eder ve dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
        - Tarih benzeri string kolonlarÄ± datetime tipine Ã§evirir.
        - SayÄ±sal deÄŸerlere benzer kolonlarÄ± float tipine Ã§evirir.
        - 'id', 'code' gibi kolonlar koruma altÄ±ndadÄ±r.
        """
        out = df.clone()

        if not datetime_guess:
            return out

        id_like = {"id", "key", "user_id", "customer_id", "kod", "code"}
        protected = set((protected_cols or []))

        # id benzeri kolonlarÄ± koru
        if protect_id_like_names:
            for c in out.columns:
                if str(c).strip().lower() in id_like:
                    protected.add(c)

        for col in out.columns:
            if col in protected:
                continue

            s = out[col]

            # sadece Utf8 tipli kolonlarÄ± dÃ¶nÃ¼ÅŸtÃ¼rmeyi dene
            if s.dtype == pl.Utf8:
                # --- Tarih tipine dÃ¶nÃ¼ÅŸtÃ¼rme ---
                dt_parsed = None
                common_formats = [
                    "%Y-%m-%d", "%d-%m-%Y", "%Y/%m/%d", "%d/%m/%Y",
                    "%Y.%m.%d", "%d.%m.%Y", "%Y%m%d",
                    "%d-%b-%Y", "%d %b %Y", "%b %d %Y",
                    "%Y-%m-%dT%H:%M:%S"
                ]

                for fmt in common_formats:
                    try:
                        dt_parsed = s.str.strptime(pl.Datetime, format=fmt, strict=False)
                        # baÅŸarÄ± oranÄ±nÄ± Ã¶lÃ§ (%50 Ã¼zeri olursa kabul et)
                        if dt_parsed.drop_nulls().len() / max(1, s.len()) > 0.5:
                            break
                    except Exception:
                        continue

                if dt_parsed is not None and dt_parsed.drop_nulls().len() / max(1, s.len()) >= coerce_threshold:
                    if normalize_tz_to_naive_utc:
                        try:
                            dt_parsed = dt_parsed.dt.replace_time_zone(None)
                        except Exception:
                            pass
                    out = out.with_columns(dt_parsed.alias(col))
                    continue

                # --- SayÄ±sal tahmin ---
                num_parsed = s.cast(pl.Float64, strict=False)
                num_valid_ratio = num_parsed.drop_nulls().len() / max(1, s.len())
                if num_valid_ratio >= coerce_threshold:
                    out = out.with_columns(num_parsed.alias(col))
                    continue

        return out

    @staticmethod
    def validate(df: pl.DataFrame, max_rows: int = 7_000_000, max_cols: int = 5_000) -> tuple[bool, str]:
        """
        DataFrame boyutlarÄ±nÄ± kontrol eder.
        Limitleri aÅŸan durumlarda False ve hata mesajÄ± dÃ¶ndÃ¼rÃ¼r.
        """
        n_rows, n_cols = df.height, len(df.columns)

        if n_rows > max_rows:
            return False, f"SatÄ±r sayÄ±sÄ± Ã§ok bÃ¼yÃ¼k: {n_rows:,} > {max_rows:,}"
        if n_cols > max_cols:
            return False, f"SÃ¼tun sayÄ±sÄ± Ã§ok bÃ¼yÃ¼k: {n_cols:,} > {max_cols:,}"

        return True, "OK"

    @staticmethod
    def _cast_series_pair(sL: pl.Series, sR: pl.Series, mode: str) -> tuple[pl.Series, pl.Series]:
        """
        Ä°ki serinin tipini belirtilen moda gÃ¶re hizalar.
        - 'string': her iki seriyi de string'e Ã§evirir.
        - 'numeric': sayÄ±sal tipe Ã§evirir.
        - 'datetime': tarih formatÄ±na Ã§evirir.
        - 'auto': Ã¶nce datetime, sonra numeric, olmazsa string.
        """
        m = (mode or "auto").lower()

        if m == "string":
            return sL.cast(pl.Utf8, strict=False), sR.cast(pl.Utf8, strict=False)

        if m == "numeric":
            return sL.cast(pl.Float64, strict=False), sR.cast(pl.Float64, strict=False)

        if m == "datetime":
            return (
                sL.str.strptime(pl.Datetime, strict=False, utc=True),
                sR.str.strptime(pl.Datetime, strict=False, utc=True),
            )

        # AUTO: Ã¶nce datetime, sonra numeric, deÄŸilse string
        try:
            dL = sL.str.strptime(pl.Datetime, strict=False, utc=True)
            dR = sR.str.strptime(pl.Datetime, strict=False, utc=True)
            if dL.drop_nulls().height / max(1, sL.height) > 0.8 and dR.drop_nulls().height / max(1, sR.height) > 0.8:
                return dL, dR
        except Exception:
            pass

        try:
            nL = sL.cast(pl.Float64, strict=False)
            nR = sR.cast(pl.Float64, strict=False)
            if nL.drop_nulls().height / max(1, sL.height) > 0.8 and nR.drop_nulls().height / max(1, sR.height) > 0.8:
                return nL, nR
        except Exception:
            pass

        return sL.cast(pl.Utf8, strict=False), sR.cast(pl.Utf8, strict=False)

    @staticmethod
    def align_dtypes_for_merge_lr(
            df_left: pl.DataFrame,
            df_right: pl.DataFrame,
            left_on: list[str],
            right_on: list[str],
            key_cast_seq: list[str] | None = None,
    ) -> tuple[pl.DataFrame, pl.DataFrame]:
        """
        Ä°ki DataFrame'deki anahtar kolonlarÄ±n tiplerini hizalar.
        Her eÅŸleÅŸme iÃ§in belirli bir cast stratejisi (auto/string/numeric/datetime) kullanÄ±labilir.
        """
        if len(left_on) != len(right_on):
            raise ValueError("left_on ve right_on uzunluklarÄ± eÅŸit olmalÄ±.")

        L, R = df_left.clone(), df_right.clone()

        if key_cast_seq is None:
            key_cast_seq = ["auto"] * len(left_on)
        if len(key_cast_seq) != len(left_on):
            raise ValueError("key_cast_seq uzunluÄŸu anahtar sayÄ±sÄ±yla eÅŸit olmalÄ±.")

        for lc, rc, mode in zip(left_on, right_on, key_cast_seq):
            if lc not in L.columns or rc not in R.columns:
                # Eksik kolon varsa string olarak varsay
                if lc in L.columns:
                    L = L.with_columns(L[lc].cast(pl.Utf8, strict=False))
                if rc in R.columns:
                    R = R.with_columns(R[rc].cast(pl.Utf8, strict=False))
                continue

            sL, sR = L[lc], R[rc]
            cL, cR = DataUtils._cast_series_pair(sL, sR, mode)
            L = L.with_columns(cL.alias(lc))
            R = R.with_columns(cR.alias(rc))

        return L, R

    @staticmethod
    def merge_safe_lr(
            df_left: pl.DataFrame,
            df_right: pl.DataFrame,
            left_on: list[str],
            right_on: list[str],
            how: str = "inner",
            suffixes: tuple[str, str] = ("_x", "_y"),
            key_cast_seq: list[str] | None = None,
    ) -> pl.DataFrame:
        """
        FarklÄ± isimli anahtar kolonlarÄ± eÅŸleÅŸtirerek gÃ¼venli bir merge iÅŸlemi gerÃ§ekleÅŸtirir.
        - left_on / right_on: eÅŸleÅŸen kolon listeleri
        - key_cast_seq: her eÅŸleÅŸme iÃ§in tip stratejisi ('auto'|'string'|'numeric'|'datetime')
        """
        if not left_on or not right_on:
            raise ValueError("left_on / right_on boÅŸ olamaz.")
        if len(left_on) != len(right_on):
            raise ValueError("left_on ve right_on uzunluklarÄ± eÅŸit olmalÄ±.")

        # Ã–nce tipleri hizala
        L, R = DataUtils.align_dtypes_for_merge_lr(df_left, df_right, left_on, right_on, key_cast_seq)

        # Polars join metodu
        # right_on kullanÄ±mÄ± Polarsâ€™ta doÄŸrudan left_on ile birlikte belirtilir
        joined = L.join(
            R,
            left_on=left_on,
            right_on=right_on,
            how=how,
            suffix=suffixes[1] if suffixes else "_right"
        )

        return joined

    @staticmethod
    def convert_column_type(df: pl.DataFrame, column: str, target_type: str) -> pl.DataFrame:
        """
        SeÃ§ili kolonu verilen hedef tÃ¼re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
        DÃ¶nÃ¼ÅŸÃ¼m, tipler arasÄ± doÄŸrudan (Ã¶rn: Datetime->Date) veya
        otomatik format tanÄ±ma (Ã¶rn: String->Date) yoluyla yapÄ±lÄ±r.
        """
        try:
            current_dtype = df[column].dtype

            # 1ï¸âƒ£ String dÃ¶nÃ¼ÅŸÃ¼mÃ¼
            if target_type == "string":
                df = df.with_columns(pl.col(column).cast(pl.Utf8))

            # 2ï¸âƒ£ SayÄ±sal dÃ¶nÃ¼ÅŸÃ¼mler
            elif target_type == "int":
                df = df.with_columns(pl.col(column).cast(pl.Int64, strict=False))
            elif target_type == "float":
                df = df.with_columns(pl.col(column).cast(pl.Float64, strict=False))

            # 3ï¸âƒ£ Boolean dÃ¶nÃ¼ÅŸÃ¼mÃ¼
            elif target_type == "boolean":
                df = df.with_columns(pl.col(column).cast(pl.Boolean, strict=False))

            # 4ï¸âƒ£ Date dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (Sadece YÄ±l-Ay-GÃ¼n)
            elif target_type == "date":
                if current_dtype == pl.Datetime:
                    # 1. Mevcut tip Datetime ise, saati sil (Verimli)
                    df = df.with_columns(pl.col(column).cast(pl.Date))  # Bu zaten doÄŸruydu
                elif current_dtype == pl.Date:
                    # 2. Zaten Date ise, dokunma
                    pass
                else:
                    # 3. DiÄŸer (string, int) tiplerden geliyorsa, OTOMATÄ°K parse et
                    df = df.with_columns(
                        pl.col(column)
                        .cast(pl.Utf8)
                        .str.strptime(pl.Datetime, format=None, strict=False)
                        .dt.date()
                    )

            # 5ï¸âƒ£ Datetime dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (Tarih + Saat)
            elif target_type == "datetime":
                if current_dtype == pl.Date:
                    # 1. Mevcut tip Date ise, saat ekle (Verimli)

                    # [HATA DÃœZELTMESÄ° 2]
                    # .dt.datetime() metodu Date tipi Ã¼zerinde Ã§alÄ±ÅŸmaz.
                    # DoÄŸru yÃ¶ntem .cast(pl.Datetime) kullanmaktÄ±r.
                    df = df.with_columns(pl.col(column).cast(pl.Datetime))

                elif current_dtype == pl.Datetime:
                    # 2. Zaten Datetime ise, dokunma
                    pass
                else:
                    # 3. DiÄŸer (string, int) tiplerden geliyorsa, OTOMATÄ°K parse et
                    df = df.with_columns(
                        pl.col(column)
                        .cast(pl.Utf8)
                        .str.strptime(pl.Datetime, format=None, strict=False)
                    )

            return df

        except Exception as e:
            raise ValueError(f"DÃ¶nÃ¼ÅŸÃ¼m hatasÄ± ({column} -> {target_type}): {e}")

    @staticmethod
    def extract_date_parts(df: pl.DataFrame, column: str) -> pl.DataFrame:
        """
        [BONUS DÃœZELTME] Docstring gÃ¼ncellendi.
        Tarih veya Tarih/Saat sÃ¼tunundan yÄ±l, ay, gÃ¼n bilgilerini Ã§Ä±karÄ±r.
        Sadece Datetime veya Date tÃ¼rÃ¼ sÃ¼tunlarda Ã§alÄ±ÅŸÄ±r.
        """
        dtype = df[column].dtype

        # 1ï¸âƒ£ Kontrol: SÃ¼tun datetime deÄŸilse anlamlÄ± uyarÄ± ver
        if dtype not in (pl.Datetime, pl.Date):
            raise TypeError(
                f"'{column}' sÃ¼tunu {dtype} tipinde. "
                f"YalnÄ±zca Datetime veya Date tÃ¼rlerinde tarih ayrÄ±ÅŸtÄ±rma yapÄ±labilir."
            )

        # 2ï¸âƒ£ GÃ¼venli dÃ¶nÃ¼ÅŸÃ¼m iÅŸlemleri
        try:
            df = df.with_columns([
                pl.col(column).dt.year().alias(f"{column}_year"),
                pl.col(column).dt.month().alias(f"{column}_month"),
                pl.col(column).dt.day().alias(f"{column}_day"),
            ])
            return df
        except Exception as e:
            raise ValueError(f"Tarih ayrÄ±ÅŸtÄ±rma hatasÄ±: {e}")

    @staticmethod
    def _bytes_to_mb(nbytes: int) -> float:
        """
        Byte deÄŸerini megabayt (MB) cinsine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
        """
        return round(nbytes / (1024 ** 2), 3)

    @staticmethod
    def profile(df: pl.DataFrame, sample_rows: int = 1000) -> DataProfile:
        """
        Polars DataFrame iÃ§in profil Ã§Ä±karÄ±mÄ± yapar.
        - SatÄ±r/sÃ¼tun sayÄ±sÄ±
        - Bellek kullanÄ±mÄ±
        - Eksik veri oranÄ±
        - Kolon tÃ¼rleri (numerik, kategorik, datetime)
        - Ã–rnek satÄ±rlar
        """
        n_rows, n_cols = df.height, len(df.columns)
        mem_mb = DataUtils._bytes_to_mb(df.estimated_size())

        # Eksik deÄŸer sayÄ±mÄ± â€” gÃ¼venli versiyon
        missing_total = int(
            df.select(pl.sum_horizontal([pl.col(c).is_null().cast(pl.Int64) for c in df.columns]))[0, 0])
        missing_ratio = float(missing_total) / float(max(1, n_rows * n_cols))

        # Tip sÄ±nÄ±flandÄ±rmasÄ±
        numeric_cols = [c for c, t in zip(df.columns, df.dtypes)
                        if t in (pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64)]

        # pl.Date tipi de Tarih/Zaman olarak sÄ±nÄ±flandÄ±rÄ±lmalÄ±.
        datetime_cols = [c for c, t in zip(df.columns, df.dtypes) if t in (pl.Datetime, pl.Date)]

        categorical_cols = [c for c in df.columns if c not in numeric_cols + datetime_cols]

        sample = df.head(sample_rows)

        return DataProfile(
            n_rows=n_rows,
            n_cols=n_cols,
            mem_usage_mb=mem_mb,
            missing_total=int(missing_total),
            missing_ratio=round(missing_ratio, 4),
            numeric_cols=numeric_cols,
            categorical_cols=categorical_cols,
            datetime_cols=datetime_cols,
            sample=sample
        )

    # --- Tek kolon profili (Variables paneli iÃ§in) -------------------------------
    @staticmethod
    def variable_profile(df: pl.DataFrame, col: str, bins: int = 40) -> dict:
        """
        Tek bir kolonun istatistiksel profilini Ã§Ä±karÄ±r.
        - SayÄ±sal kolonlar iÃ§in: min, max, mean, std, histogram
        - Tarih kolonlarÄ± iÃ§in: min ve max tarih
        - Kategorik kolonlar iÃ§in: distinct oranÄ±
        """
        s = df[col]
        n = s.len()
        dtype = str(s.dtype)
        mem_mb = DataUtils._bytes_to_mb(s.estimated_size())

        # Eksik ve distinct metrikleri
        missing = int(s.null_count())
        missing_pct = round((missing / max(1, n)) * 100, 4)
        if s.dtype == pl.Utf8 and n > 500_000:
            distinct = int(s.approx_n_unique())
        else:
            distinct = int(s.n_unique())
        distinct_pct = round((distinct / max(1, n)) * 100, 4)

        out = {
            "dtype": dtype,
            "n": n,
            "missing": missing,
            "missing_pct": missing_pct,
            "distinct": distinct,
            "distinct_pct": distinct_pct,
            "mem_mb": round(mem_mb, 3),
            "min": None, "max": None, "mean": None, "std": None,
            "zeros": None, "zeros_pct": None,
            "neg": None, "neg_pct": None,
            "hist": None, "hist_edges": None
        }

        # --- SayÄ±sal kolonlar ---
        if s.dtype in (pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64):
            s_nonnull = s.drop_nulls()
            if s_nonnull.len() > 0:
                out["min"] = float(s_nonnull.min())
                out["max"] = float(s_nonnull.max())
                out["mean"] = float(s_nonnull.mean())
                out["std"] = float(s_nonnull.std())

                zeros = int((s_nonnull == 0).sum())
                neg = int((s_nonnull < 0).sum())
                out["zeros"], out["neg"] = zeros, neg
                out["zeros_pct"] = (zeros / max(1, n)) * 100
                out["neg_pct"] = (neg / max(1, n)) * 100

                # Histogram (numpy uyumlu)
                import numpy as np
                h, edges = np.histogram(s_nonnull.to_numpy(), bins=bins)
                out["hist"], out["hist_edges"] = h.tolist(), edges.tolist()

        # --- Tarih kolonlarÄ± ---
        elif s.dtype == pl.Datetime:
            s_nonnull = s.drop_nulls()
            if s_nonnull.len() > 0:
                out["min"] = s_nonnull.min()
                out["max"] = s_nonnull.max()

        return out

    @staticmethod
    def variable_common_values(df: pl.DataFrame, col: str, top: int = 20) -> pl.DataFrame:
        """
        Kolondaki en sÄ±k gÃ¶rÃ¼len 'top' deÄŸerleri ve yÃ¼zdelik oranlarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
        'Other values' satÄ±rÄ±nda kalan tÃ¼m deÄŸerler Ã¶zetlenir.
        """
        s = df[col].cast(pl.Utf8).fill_null("NA")
        total = s.len()

        # DeÄŸerlerin frekans sayÄ±mÄ±
        vc = s.value_counts(sort=True)
        top_vc = vc.head(top)

        top_count_sum = int(top_vc["count"].sum()) if top_vc.height > 0 else 0
        others_count = int(total - top_count_sum) if vc.height > top else 0

        # Frekans oranlarÄ±nÄ± hesapla
        out = top_vc.with_columns(
            (pl.col("count") / max(1, total) * 100).alias("freq_pct")
        ).rename({col: "value"})

        # âœ… TÃ¼m kolon tiplerini Int64 + Float64 olarak sabitle
        out = out.with_columns([
            pl.col("count").cast(pl.Int64),
            pl.col("freq_pct").cast(pl.Float64)
        ])

        # 'Other values' satÄ±rÄ±nÄ± ekle
        if others_count > 0:
            other_row = pl.DataFrame({
                "value": ["Other values"],
                "count": [others_count],
                "freq_pct": [others_count / max(1, total) * 100]
            })

            # AynÄ± ÅŸemayÄ± korumak iÃ§in cast et
            other_row = other_row.with_columns([
                pl.col("count").cast(pl.Int64),
                pl.col("freq_pct").cast(pl.Float64)
            ])

            out = pl.concat([out, other_row])

        return out

    # --- Tek kolon iÃ§in tablo hazÄ±r istatistikler (Variables/Statistics sekmesi) ---
    @staticmethod
    def variable_quantile_table(s: pl.Series) -> pl.DataFrame:
        """
        SayÄ±sal seriler iÃ§in quantile (daÄŸÄ±lÄ±m) Ã¶zet tablosu dÃ¶ndÃ¼rÃ¼r.
        - Minimum, Q1, Median, Q3, Maksimum
        - Range ve IQR hesaplarÄ± dahil
        """
        s = s.cast(pl.Float64, strict=False).drop_nulls()

        if s.is_empty():
            return pl.DataFrame({"": [], "value": []})

        q = {
            "Minimum": s.min(),
            "5-th percentile": s.quantile(0.05, interpolation="nearest"),
            "Q1": s.quantile(0.25, interpolation="nearest"),
            "median": s.median(),
            "Q3": s.quantile(0.75, interpolation="nearest"),
            "95-th percentile": s.quantile(0.95, interpolation="nearest"),
            "Maximum": s.max(),
        }

        q["Range"] = q["Maximum"] - q["Minimum"]
        q["Interquartile range (IQR)"] = q["Q3"] - q["Q1"]

        df = pl.DataFrame({
            "": list(q.keys()),
            "value": [float(v) if v is not None else None for v in q.values()]
        })

        return df

    @staticmethod
    def variable_descriptive_table(s: pl.Series) -> pl.DataFrame:
        """
        SayÄ±sal seriler iÃ§in tanÄ±mlayÄ±cÄ± istatistik tablosu oluÅŸturur.
        - Ortalama, varyans, standart sapma, Ã§arpÄ±klÄ±k, basÄ±klÄ±k vb.
        """
        s = s.cast(pl.Float64, strict=False).drop_nulls()

        if s.is_empty():
            return pl.DataFrame({"": [], "value": []})

        std = s.std()
        mean = s.mean()
        variance = s.var()
        sum_val = s.sum()
        mad = (s - s.median()).abs().median()

        n = s.len()
        if n > 2:
            centered = s - mean
            skew = float((centered ** 3).mean() / (std ** 3)) if std not in (0, None) else None
            kurt = float((centered ** 4).mean() / (std ** 4)) - 3 if std not in (0, None) else None
        else:
            skew, kurt = None, None

        is_inc = s.is_sorted()
        is_dec = s[::-1].is_sorted()
        monotonicity = (
            "Monotonic increasing" if is_inc else
            ("Monotonic decreasing" if is_dec else "Not monotonic")
        )

        desc = {
            "Standard deviation": std,
            "Coefficient of variation (CV)": (std / mean) if mean not in (0, None) else None,
            "Kurtosis": kurt,
            "Mean": mean,
            "Median Absolute Deviation (MAD)": mad,
            "Skewness": skew,
            "Sum": sum_val,
            "Variance": variance,
            "Monotonicity": monotonicity,
        }

        # âœ… Polars strict=False veya tÃ¼mÃ¼nÃ¼ stringe Ã§evir
        df = pl.DataFrame({
            "": list(desc.keys()),
            "value": [str(v) if v is not None else "â€”" for v in desc.values()]
        })

        return df

    @staticmethod
    def correlation_matrix(df: pl.DataFrame) -> pl.DataFrame:
        """
        SayÄ±sal deÄŸiÅŸkenler iÃ§in korelasyon matrisini dÃ¶ndÃ¼rÃ¼r (Polars vektÃ¶rel).
        """
        # Sadece sayÄ±sal sÃ¼tunlarÄ± seÃ§
        numeric_cols = [c for c, dtype in zip(df.columns, df.dtypes)
                        if dtype in (pl.Float32, pl.Float64, pl.Int32, pl.Int64)]

        if not numeric_cols:
            return pl.DataFrame({"column": [], "message": ["SayÄ±sal deÄŸiÅŸken bulunamadÄ±."]})

        # Polars 0.20+ sÃ¼rÃ¼mÃ¼ iÃ§in correlation_matrix
        corr = df.select(numeric_cols).to_pandas().corr(method="pearson")
        corr_df = pl.DataFrame(corr.reset_index(names="column"))
        return corr_df

    @staticmethod
    def missing_value_summary(df: pl.DataFrame) -> pl.DataFrame:
        """
        Her kolon iÃ§in eksik deÄŸer sayÄ±sÄ± ve oranÄ±nÄ± hesaplar (Polars vektÃ¶rel).
        """
        n_rows = df.height

        summary = (
            df.select([
                pl.col(c).is_null().sum().alias(c)
                for c in df.columns
            ])
            .transpose(include_header=True, header_name="column", column_names=["missing_count"])
            .with_columns([
                (pl.col("missing_count") / n_rows * 100).alias("missing_pct")
            ])
            .sort("missing_pct", descending=True)
        )
        return summary

    # Eksik DeÄŸerlerin DoldurulmasÄ±
    @staticmethod
    def get_missing_columns(df: pl.DataFrame) -> list[str]:
        """
        Eksik deÄŸer (null) iÃ§eren kolonlarÄ± dÃ¶ndÃ¼rÃ¼r.
        """
        null_counts = df.null_count().to_dicts()[0]
        return [c for c, v in null_counts.items() if v > 0]

    # === ğŸ§© 2. TÃ¼m Doldurma YÃ¶ntemleri (Tek Nokta TanÄ±mÄ±) ===
    @staticmethod
    def get_fill_methods() -> dict[str, str]:
        """
        Mevcut tÃ¼m doldurma yÃ¶ntemlerini (anahtar + aÃ§Ä±klama) dÃ¶ndÃ¼rÃ¼r.
        [GÃœNCELLEME] Model bazlÄ± yÃ¶ntemler eklendi.
        """
        return {
            # Temel YÃ¶ntemler
            "custom": "âœï¸ Sabit (manuel) deÄŸerle doldur",
            "forward": "â¡ï¸ Ä°leri yÃ¶nlÃ¼ doldur (ffill)",
            "backward": "â¬…ï¸ Geri yÃ¶nlÃ¼ doldur (bfill)",
            "mode": "ğŸ” Mod (en sÄ±k gÃ¶rÃ¼len) ile doldur",

            # SayÄ±sal - Basit
            "mean": "ğŸ“Š Ortalama ile doldur",
            "median": "ğŸ“ˆ Medyan ile doldur",
            "zero": "0ï¸âƒ£ SÄ±fÄ±r (0) ile doldur",
            "min": "ğŸ”½ Minimum deÄŸerle doldur",
            "max": "ğŸ”¼ Maksimum deÄŸerle doldur",

            # SayÄ±sal - GeliÅŸmiÅŸ
            "interpolate_linear": "ğŸ“ˆ DoÄŸrusal Ä°nterpolasyon (SÄ±ralÄ±)",
            "knn_imputer": "ğŸ¤ K-NN Imputer (Model BazlÄ±)",
            "iterative_imputer": "ğŸ§  Iterative Imputer (MICE, Model BazlÄ±)",
        }

    # === ğŸ§© 3. Tip BazlÄ± Uygun YÃ¶ntem Ã–nerisi ===
    @staticmethod
    def suggest_fill_methods(dtype: pl.DataType) -> list[str]:
        """
        Veri tipine gÃ¶re uygulanabilir doldurma yÃ¶ntemlerini dÃ¶ndÃ¼rÃ¼r.
        [GÃœNCELLEME] GeliÅŸmiÅŸ yÃ¶ntemler eklendi.
        """
        # TÃ¼m tipler iÃ§in geÃ§erli temel yÃ¶ntemler
        base_methods = ["mode", "custom", "forward", "backward"]

        if dtype in (pl.Int64, pl.Float64):
            # SayÄ±sal yÃ¶ntemler + Temel yÃ¶ntemler
            numeric_methods = [
                "mean", "median", "min", "max", "zero",
                "interpolate_linear", "knn_imputer", "iterative_imputer"
            ]
            return numeric_methods + base_methods

        elif dtype in (pl.Utf8, pl.Boolean):
            # Kategorik/Boolean iÃ§in sadece temel yÃ¶ntemler mantÄ±klÄ±
            return base_methods

        elif dtype in (pl.Date, pl.Datetime):
            # Tarih iÃ§in (mean, zero vb. mantÄ±ksÄ±z)
            return base_methods

        else:
            # DiÄŸer tÃ¼m tipler (binary, list vb.)
            return ["custom"]

    # === ğŸ§© 4. Doldurma DeÄŸeri Hesaplama (Metoda GÃ¶re) ===
    @staticmethod
    def compute_fill_value(df: pl.DataFrame, column: str, method: str, custom_value=None):
        """
        Kolon ve seÃ§ilen metoda gÃ¶re doldurma deÄŸerini hesaplar.
        None dÃ¶nerse doldurma yapÄ±lmaz (Ã¶rneÄŸin tÃ¼m deÄŸerler null ise).
        """
        s = df[column]

        if s.null_count() == len(s):
            # Kolon tamamen boÅŸsa hiÃ§bir ÅŸey yapÄ±lmaz
            return None

        if method == "mean":
            val = s.mean()
        elif method == "median":
            val = s.median()
        elif method == "mode":
            modes = s.drop_nulls().mode().to_list()
            val = modes[0] if modes else None
        elif method == "min":
            val = s.min()
        elif method == "max":
            val = s.max()
        elif method == "zero":
            val = 0
        elif method in ("specific", "custom"):
            val = custom_value
        else:
            raise ValueError(f"Desteklenmeyen doldurma yÃ¶ntemi: {method}")

        # EÄŸer sonuÃ§ hala None ise, None dÃ¶ndÃ¼r (fill_missing uyarÄ± verecek)
        return val

    @staticmethod
    def fill_missing(df: pl.DataFrame, column: str, method: str, custom_value=None) -> pl.DataFrame:
        """
        SeÃ§ilen kolonun eksik deÄŸerlerini belirtilen metoda gÃ¶re doldurur.
        Polars 1.x uyumludur.
        [GÃœNCELLEME] Polars native, Sklearn (K-NN/MICE) ve basit yÃ¶ntemleri destekler.
        """
        col_expr = pl.col(column)

        try:
            # === 1ï¸âƒ£ Polars Native YÃ¶ntemler (HÄ±zlÄ±) ===
            # (ffill/bfill/interpolate)
            if method == "forward":
                expr = col_expr.forward_fill()
                return df.with_columns(expr)

            elif method == "backward":
                expr = col_expr.backward_fill()
                return df.with_columns(expr)

            elif method == "interpolate_linear":
                # Sadece sayÄ±sal kolonlarda Ã§alÄ±ÅŸÄ±r
                if df[column].dtype not in pl.NUMERIC_DTYPES:
                    raise TypeError("DoÄŸrusal interpolasyon sadece sayÄ±sal kolonlarda Ã§alÄ±ÅŸÄ±r.")
                expr = col_expr.interpolate(method="linear")
                return df.with_columns(expr)

            # === 2ï¸âƒ£ Sklearn Model BazlÄ± YÃ¶ntemler (YavaÅŸ, Pandas dÃ¶nÃ¼ÅŸÃ¼mÃ¼) ===
            # (K-NN / MICE)
            elif method in ("knn_imputer", "iterative_imputer"):

                # Bu yÃ¶ntemler tahmin iÃ§in *diÄŸer* sayÄ±sal kolonlarÄ± kullanÄ±r.
                numeric_cols = [c for c, t in zip(df.columns, df.dtypes) if t in pl.NUMERIC_DTYPES]

                if len(numeric_cols) < 2:
                    raise ValueError(
                        f"'{method}' yÃ¶ntemi, tahmin yapabilmek iÃ§in en az bir baÅŸka sayÄ±sal kolona daha ihtiyaÃ§ duyar.")

                # Sadece sayÄ±sal veriyi Pandas'a Ã§evir
                df_pd_numeric = df.select(numeric_cols).to_pandas()

                # Orijinal kolon isimlerini ve indeksi koru
                original_index = df_pd_numeric.index
                original_columns = df_pd_numeric.columns

                if method == "knn_imputer":
                    # K-NN iÃ§in Ã¶lÃ§eklendirme (scaling) zorunludur
                    scaler = StandardScaler()
                    df_scaled = scaler.fit_transform(df_pd_numeric)

                    imputer = KNNImputer(n_neighbors=5)
                    df_imputed_scaled = imputer.fit_transform(df_scaled)

                    # Ã–lÃ§eklendirmeyi geri al
                    df_imputed_unscaled = scaler.inverse_transform(df_imputed_scaled)
                    df_imputed_pd = pd.DataFrame(df_imputed_unscaled,
                                                 columns=original_columns,
                                                 index=original_index)

                else:  # iterative_imputer (MICE)
                    # MICE (regresyon bazlÄ±) Ã¶lÃ§eklendirme gerektirmez
                    imputer = IterativeImputer(max_iter=10, random_state=0)
                    df_imputed_values = imputer.fit_transform(df_pd_numeric)
                    df_imputed_pd = pd.DataFrame(df_imputed_values,
                                                 columns=original_columns,
                                                 index=original_index)

                # DoldurulmuÅŸ Pandas verisini Polars'a geri Ã§evir
                df_filled_pl = pl.from_pandas(df_imputed_pd, include_index=False)

                # Orijinal Polars DataFrame'ini, doldurulan sayÄ±sal kolonlarla gÃ¼ncelle
                # Bu, sayÄ±sal olmayan (kategorik, tarih) kolonlarÄ± korur.
                return df.update(df_filled_pl)

            # === 3ï¸âƒ£ Basit YÃ¶ntemler (compute_fill_value) ===
            # (mean, median, mode, zero, custom vb.)
            else:
                fill_val = DataUtils.compute_fill_value(df, column, method, custom_value)

                if fill_val is None:
                    # EÄŸer hesaplanabilir bir deÄŸer yoksa iÅŸlem yapma
                    st.warning(f"'{column}' iÃ§in {method} yÃ¶ntemiyle doldurma deÄŸeri hesaplanamadÄ±. "
                               f"Kolon tamamen boÅŸ olabilir.")
                    return df  # DeÄŸiÅŸiklik yapma

                expr = col_expr.fill_null(fill_val)
                return df.with_columns(expr)

        except Exception as e:
            raise ValueError(f"'{column}' sÃ¼tununda '{method}' yÃ¶ntemiyle doldurma hatasÄ±: {e}")

    @staticmethod
    def drop_columns(df: pl.DataFrame, cols: list[str]) -> pl.DataFrame:
        """
        Verilen kolonlarÄ± KESÄ°N olarak siler.
        - cols boÅŸsa dokunmaz.
        - DF'te bulunmayan bir kolon varsa HATA verir.
        """
        if not cols:
            return df

        missing = [c for c in cols if c not in df.columns]
        if missing:
            raise ValueError(f"Bulunamayan sÃ¼tun(lar): {missing}")

        return df.drop(cols)

    @staticmethod
    def quantile_bounds_summary(
            df: pl.DataFrame,
            cols: list[str],
            q_low: float = 0.25,
            q_high: float = 0.75,
            keep_nulls: bool = True,
    ) -> pl.DataFrame:
        """
        SeÃ§ili sayÄ±sal sÃ¼tunlar iÃ§in alt/Ã¼st yÃ¼zdelik deÄŸerlerini ve aralÄ±ÄŸa gÃ¶re
        satÄ±r daÄŸÄ±lÄ±mlarÄ±nÄ± Ã¶zetler. (Filtre uygulamaz)
        DÃ¶nÃ¼ÅŸ: pl.DataFrame:
          column | q_low | q_high | q_low_val | q_high_val | in_range | below | above | nulls
        """
        if not cols:
            return pl.DataFrame({
                "column": [], "q_low": [], "q_high": [],
                "q_low_val": [], "q_high_val": [],
                "in_range": [], "below": [], "above": [], "nulls": []
            })

        if not (0.0 <= q_low < q_high <= 1.0):
            raise ValueError("q_low ve q_high 0-1 aralÄ±ÄŸÄ±nda olmalÄ± ve q_low < q_high olmalÄ±.")

        # Sadece sayÄ±sal sÃ¼tunlarÄ± iÅŸle
        numeric_types = (
            pl.Int8, pl.Int16, pl.Int32, pl.Int64,
            pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
            pl.Float32, pl.Float64
        )
        use_cols = [c for c in cols if df.schema.get(c) in numeric_types]
        if not use_cols:
            raise ValueError("SeÃ§ilen sÃ¼tunlarÄ±n hiÃ§biri sayÄ±sal deÄŸil.")

        rows = []
        n = df.height
        for c in use_cols:
            s = df[c].cast(pl.Float64, strict=False)
            s_nonnull = s.drop_nulls()
            if s_nonnull.is_empty():
                # TamamÄ± null ise anlamlÄ± eÅŸik Ã¼retemez; yine de Ã§Ä±kÄ±ÅŸa ekleyelim
                rows.append({
                    "column": c, "q_low": q_low, "q_high": q_high,
                    "q_low_val": None, "q_high_val": None,
                    "in_range": 0, "below": 0, "above": 0, "nulls": int(s.null_count())
                })
                continue

            lo = float(s_nonnull.quantile(q_low))
            hi = float(s_nonnull.quantile(q_high))
            # KapsayÄ±cÄ± aralÄ±k [lo, hi]
            in_range_mask = s.is_between(lo, hi, closed="both")
            if keep_nulls:
                in_range_mask = in_range_mask | s.is_null()

            in_range = int(in_range_mask.sum())
            nulls = int(s.is_null().sum())
            below = int((s < lo).sum())
            above = int((s > hi).sum())

            rows.append({
                "column": c, "q_low": q_low, "q_high": q_high,
                "q_low_val": lo, "q_high_val": hi,
                "in_range": in_range, "below": below, "above": above, "nulls": nulls
            })

        return pl.DataFrame(rows)

    @staticmethod
    def remove_outliers_quantile(
            df: pl.DataFrame,
            cols: list[str],
            q_low: float = 0.25,
            q_high: float = 0.75,
            how: str = "any",  # "any" => herhangi bir seÃ§ili kolonda [lo,hi] dÄ±ÅŸÄ±nda ise satÄ±rÄ± sil
            # "all" => tÃ¼m seÃ§ili kolonlarda [lo,hi] dÄ±ÅŸÄ±nda ise sil
            keep_nulls: bool = True,  # True => null hÃ¼creler filtreyi geÃ§er
            return_summary: bool = True,
    ):
        """
        YÃ¼zdelik aralÄ±ÄŸÄ±na gÃ¶re satÄ±rlarÄ± temizler.
        AralÄ±k: [q_low, q_high] yÃ¼zdeliklerinin deÄŸerleri (kapsayÄ±cÄ±).
        DÃ¶nÃ¼ÅŸ: df_filtered (ve return_summary=True ise summary DF)
        """
        if not cols:
            return (df, DataUtils.quantile_bounds_summary(df, [], q_low, q_high, keep_nulls)) if return_summary else df

        if not (0.0 <= q_low < q_high <= 1.0):
            raise ValueError("q_low ve q_high 0-1 aralÄ±ÄŸÄ±nda olmalÄ± ve q_low < q_high olmalÄ±.")

        # Ã–zet ve eÅŸikler
        summary = DataUtils.quantile_bounds_summary(df, cols, q_low, q_high, keep_nulls=False)

        # KoÅŸullarÄ± hazÄ±rla (Ã¶zet DF'ten lo/hi Ã§ek)
        conds = []
        for row in summary.iter_rows(named=True):
            c = row["column"]
            lo = row["q_low_val"]
            hi = row["q_high_val"]
            # Null quantile (tÃ¼mÃ¼ null vs.) ise bu kolonu yok say
            if lo is None or hi is None or not np.isfinite(lo) or not np.isfinite(hi):
                continue
            in_range = pl.col(c).cast(pl.Float64, strict=False).is_between(lo, hi, closed="both")
            conds.append(in_range | pl.col(c).is_null() if keep_nulls else in_range)

        if not conds:
            # EÅŸik Ã¼retilemediyse dokunma
            return (df, summary) if return_summary else df

        # any -> tÃ¼m kolonlarda "in_range" koÅŸullarÄ±nÄ± AND'le (biri dÄ±ÅŸÄ±ndaysa satÄ±rÄ± kaldÄ±r)
        # all -> en az birinde "in_range" ise kalsÄ±n; hiÃ§biri deÄŸilse kaldÄ±r (yani OR)
        mask = (pl.all_horizontal(conds) if how == "any" else pl.any_horizontal(conds))
        df_new = df.filter(mask)

        return (df_new, summary) if return_summary else df_new

    @staticmethod
    def _value_counts(df: pl.DataFrame, col: str, cast_to_utf8: bool = True) -> pl.DataFrame:
        s = df[col]
        if cast_to_utf8:
            s = s.cast(pl.Utf8, strict=False)
        vc = s.value_counts(sort=True)  # -> DataFrame: [col, "count"]
        total = int(vc["count"].sum())
        vc = vc.with_columns((pl.col("count") / total).alias("freq"))
        return vc

    @staticmethod
    def rare_summary(
            df: pl.DataFrame,
            cols: List[str],
            *,
            min_count: Optional[int] = None,  # Ã¶rn. < 10
            min_freq: Optional[float] = None,  # 0-1 arasÄ± (Ã¶rn. < 0.01 = %1)
            top_k: Optional[int] = None,  # Ã¶rn. ilk 10 kalsÄ±n, diÄŸerleri "DiÄŸer"
            other_label: str = "DiÄŸer",
            cast_to_utf8: bool = True,  # tip Ã§akÄ±ÅŸmalarÄ±nÄ± Ã¶nlemek iÃ§in string'e taÅŸÄ±
            rare_examples_limit: int = 5,
    ) -> pl.DataFrame:
        """
        UYGULAMA YAPMADAN Ã¶zet Ã¼retir.
        Kolon bazÄ±nda: toplam satÄ±r, benzersiz deÄŸer sayÄ±sÄ±, 'rare' grubuna dÃ¼ÅŸecek kategori adedi/satÄ±r adedi vb.
        DÃ¶nÃ¼ÅŸ DF kolonlarÄ±:
          column | criterion | threshold | unique_total | unique_keep | unique_rare
                 | rows_keep | rows_rare | other_label | rare_examples
        """
        if not cols:
            return pl.DataFrame({
                "column": [], "criterion": [], "threshold": [], "unique_total": [],
                "unique_keep": [], "unique_rare": [], "rows_keep": [], "rows_rare": [],
                "other_label": [], "rare_examples": []
            })

        rows = []
        for c in cols:
            vc = DataUtils._value_counts(df, c, cast_to_utf8=cast_to_utf8)  # [c, count, freq]
            if vc.height == 0:
                rows.append({
                    "column": c, "criterion": None, "threshold": None,
                    "unique_total": 0, "unique_keep": 0, "unique_rare": 0,
                    "rows_keep": 0, "rows_rare": 0, "other_label": other_label,
                    "rare_examples": ""
                })
                continue

            crit = "top_k" if top_k is not None else ("min_count" if min_count is not None else "min_freq")
            if top_k is not None:
                keep_df = vc.sort("count", descending=True).head(top_k)
                threshold_val = top_k
            elif min_count is not None:
                keep_df = vc.filter(pl.col("count") >= min_count)
                threshold_val = min_count
            else:
                if min_freq is None:
                    raise ValueError("min_count, min_freq veya top_k parametrelerinden en az biri verilmelidir.")
                keep_df = vc.filter(pl.col("freq") >= min_freq)
                threshold_val = float(min_freq)

            keep_values = set(keep_df[c].to_list())
            all_values = set(vc[c].to_list())
            rare_values = list(all_values - keep_values)
            # Ã–rnekler
            rare_examples = ", ".join([str(x) for x in rare_values[:rare_examples_limit]])

            rows_keep = int(vc.filter(pl.col(c).is_in(list(keep_values)))["count"].sum())
            rows_rare = int(vc.filter(~pl.col(c).is_in(list(keep_values)))["count"].sum())

            rows.append({
                "column": c,
                "criterion": crit,
                "threshold": threshold_val,
                "unique_total": vc.height,
                "unique_keep": len(keep_values),
                "unique_rare": len(rare_values),
                "rows_keep": rows_keep,
                "rows_rare": rows_rare,
                "other_label": other_label,
                "rare_examples": rare_examples
            })

        return pl.DataFrame(rows)

    @staticmethod
    def rare_collapse(
            df: pl.DataFrame,
            cols: List[str],
            *,
            min_count: Optional[int] = None,
            min_freq: Optional[float] = None,
            top_k: Optional[int] = None,
            other_label: str = "DiÄŸer",
            cast_to_utf8: bool = True,
            return_summary: bool = True
    ) -> Tuple[pl.DataFrame, Optional[pl.DataFrame]]:
        """
        Az gÃ¶rÃ¼len kategorileri 'other_label' altÄ±nda toplar.
        En az bir kriter verilmelidir (min_count | min_freq | top_k).
        """
        if not cols:
            return (df, None) if return_summary else (df, None)

        # Ã–n Ã¶zet (eÅŸikleri ve rare setlerini tÃ¼retmek iÃ§in)
        summary = DataUtils.rare_summary(
            df, cols, min_count=min_count, min_freq=min_freq, top_k=top_k,
            other_label=other_label, cast_to_utf8=cast_to_utf8
        )

        df_new = df
        for row in summary.iter_rows(named=True):
            c = row["column"]
            vc = DataUtils._value_counts(df_new, c, cast_to_utf8=cast_to_utf8)

            # Keep set
            if row["criterion"] == "top_k":
                keep_df = vc.sort("count", descending=True).head(int(row["threshold"]))
            elif row["criterion"] == "min_count":
                keep_df = vc.filter(pl.col("count") >= int(row["threshold"]))
            else:
                keep_df = vc.filter(pl.col("freq") >= float(row["threshold"]))

            keep_values = set(keep_df[c].to_list())

            # DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ ifade
            base_expr = pl.col(c).cast(pl.Utf8, strict=False) if cast_to_utf8 else pl.col(c)
            expr = (
                pl.when(base_expr.is_in(list(keep_values)))
                .then(base_expr)
                .otherwise(pl.lit(other_label))
                .alias(c)
            )
            df_new = df_new.with_columns(expr)

        return (df_new, summary if return_summary else None)
